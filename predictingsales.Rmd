---
title: "Predicting sales"
output: html_document
---
#Team Members
Rachel Brigell, Emily Gao, Nina Punukollu

#Research question
Do factors associated with conventional wisdom about retail sales help to predict retail sales?

#Introduction
As graduate students on tight budgets, we are constantly looking for the best deals on clothing. Clothing stores often have sales, but it’s hard to predict when those sales will take place. Conventional wisdom suggests that factors such as temperature, macroeconomic fluctuations, and holidays are reasonable predictors of retail sales. In this project, we gathered data on these types of factors and utilized basic modelling techniques and cross validation techniques to see if they were able to predict future retail sales. With the data and methods we used, we were unable to find associations between typical “conventional wisdom” factors and retail sales at four stores: Express, LOFT, Gap, and J Crew. 

```{r, echo=FALSE}
rm(list = ls())
library(httr)
library(plyr)
library(dplyr)
library(knitr)
library(readr)
library(tidyr)
library(stringr)
library(ggplot2)
library(gridExtra)
library(ggrepel)
library(lubridate)
library(rwunderground)
library(tidytext)
library(caret)
library(timeDate)
library(data.table)
require(data.table)
```

#1. Load and wrangle the data 

#Data
Sales: To obtain a proxy for sales that are occurring at these stores, we used the Twitter API to obtain data from their accounts and used keyword analysis to identify days where they had sales. We attempted to select stores that had comparable styles of clothing and types of consumers. We did not distinguish between online and in-store sales. 

Weather: We used the WeatherUnderground API to obtain temperature and precipitation data from 4 major cities in different regions of the United States: LA, New York, Chicago, and Houston. 

Unemployment: We obtained monthly unemployment statistics from the US Bureau of Labor Statistics 

Stock prices: We obtained daily stock prices for the stores that are publically listed from Yahoo Finance. 

Holidays: We obtained US Holiday data from the R timeDate package. 

```{r}

## Weather Wrangling - Creates One File for Weather
#######################################################################

# read in weather csv files for 4 cities
new_york <- read_csv("weather_NewYork_2013-2016.csv")
los_angeles <- read_csv("weather_LA_2013-2016.csv")
chicago <- read_csv("weather_Chicago_2013-2016.csv")
houston <- read_csv("weather_Houston_2013-2016.csv")

# keep date, snow, mean_temp, precipitation columns, rename columns
new_york <- new_york %>% select(date, snow, mean_temp, precip) %>%
  rename (ny_snow = snow, ny_mean_temp = mean_temp, 
          ny_precip = precip)

los_angeles <- los_angeles %>% select(date, snow, mean_temp, precip) %>%
  rename (la_snow = snow, la_mean_temp = mean_temp, 
          la_precip = precip)

chicago <- chicago %>% select(date, snow, mean_temp, precip) %>%
  rename (chi_snow = snow, chi_mean_temp = mean_temp, 
          chi_precip = precip)

houston <- houston %>% select(date, snow, mean_temp, precip) %>%
  rename (hou_snow = snow, hou_mean_temp = mean_temp, 
          hou_precip = precip)

# join weather data from 4 cities together
ny_la <- full_join(new_york, los_angeles, by = "date")
ny_la_chi <- full_join(ny_la, chicago, by = "date")
weather <- full_join(ny_la_chi, houston, by = "date")

## Stock Wrangling - Create individual store files for stock prices
#######################################################################

# read in LOFT stock data
loft_URL <- "http://ichart.finance.yahoo.com/table.csv?s=ASNA"
loft <- read.csv(loft_URL)

# convert the date from a string to a Date type
loft$Date <- as.Date(loft$Date, "%Y-%m-%d")

# rename columns
loft_stocks <- loft %>% select(Date, Adj.Close) %>% 
  rename (date = Date, loft_adj_close = Adj.Close)

####

# read in Express stock data 
exp_URL <- "http://ichart.finance.yahoo.com/table.csv?s=EXPR"
exp <- read.csv(exp_URL)

# convert the date from a string to a Date type
exp$Date <- as.Date(exp$Date, "%Y-%m-%d")

# rename columns
exp_stocks <- exp %>% select(Date, Adj.Close) %>% 
  rename (date = Date, expr_adj_close = Adj.Close)

####

# read in Gap stock data 
gap_URL <- "http://ichart.finance.yahoo.com/table.csv?s=GPS"
gap <- read.csv(gap_URL)

# convert the date from a string to a Date type
gap$Date <- as.Date(gap$Date, "%Y-%m-%d")

# rename columns
gap_stocks <- gap %>% select(Date, Adj.Close) %>%
  rename (date = Date, gap_adj_close = Adj.Close)

## Unemployment Wrangling - Create one file for unemployment data
#######################################################################

# read in unemployment data
unemp_wide <- read_csv("Unemployment_Labor_ByMonth.csv")

# rename month columns from abbreviations to numbers
unemp_wide <- unemp_wide %>% rename("01" = Jan, "02" = Feb, "03" = Mar, "04" = Apr,
                                    "05" = May, "06" = Jun, "07" = Jul, "08" = Aug,
                                    "09" = Sep, "10" = Oct,"11" = Nov, "12" = Dec)

# convert unemployement data from wide to long
unemp <- unemp_wide %>% gather(month, unemployment, 2:13)

# create column for year-month
unemp$date_month = paste(unemp$Year, unemp$month, sep="-")

# rename date column and select only year-month and unemployment columns
unemp <- unemp %>% rename(date_y_m = date_month) %>% select(date_y_m, unemployment) %>%
  mutate(date_y_m = as.character(date_y_m))

## Join Unemployment Data + Weather + Holidays
#######################################################################

# remove time from weather date column
weather <- weather %>% mutate (date = as.Date(date, format = "%Y/%m/%d"))

# create column in weather to join with unemployment data (year-month)
weather <- weather %>% mutate(date2 = date) %>% 
  separate(date2, c("year", "month", "day"), sep = "-", convert = TRUE, fill = "right") %>% 
  mutate(month = ifelse(month < 10, paste(0, month, sep = ""), month)) %>%
  mutate(date_y_m = paste(as.character(year), as.character(month), sep = "-")) %>%
  select(-c(year, month, day))

# join weather data and unemployment data
weather_unemp <- left_join(weather, unemp, by = "date_y_m")

# remove year-month column
weather_unemp <- weather_unemp %>% select(-date_y_m)

# add in holiday data

# create vector of US holidays from 2013 - 2016
holidays <- c(
  USChristmasDay(2012:2016),
  ChristmasEve(2012:2016),
  Easter(2012:2016),
  USGoodFriday(2012:2016),
  USLaborDay(2012:2016),
  USNewYearsDay(2012:2016),
  USColumbusDay(2012:2016),
  USMemorialDay(2012:2016),
  USElectionDay(2012:2016),
  USIndependenceDay(2012:2016),
  USMLKingsBirthday(2012:2016),
  USPresidentsDay(2012:2016),
  USThanksgivingDay(2012:2016),
  (USThanksgivingDay(2012) + 1),
  (USThanksgivingDay(2013) + 1),
  (USThanksgivingDay(2014) + 1),
  (USThanksgivingDay(2015) + 1),
  (USThanksgivingDay(2016) + 1),
  USVeteransDay(2012:2016),
  BoxingDay(2012:2016)
)

# convert holiday vector into data frame & create indicator column
holidays <- as.data.frame(holidays) %>% mutate(holiday = 1)

# rename first column as date
colnames(holidays)[1] <- "date"

# convert date column into date format
holidays[1] <- as.Date(holidays$date)

weather_unemp <- left_join(weather_unemp, holidays, by = "date")

weather_unemp <- weather_unemp %>% mutate(holiday = ifelse(is.na(holiday),0,holiday))


## Wrangle Tweets -- Separate tweets into individual words 
#######################################################################

# create list of words to identify sales
sale_words = c("off", "sale")

# EXPRESS
# get tweets
express_tweets <- read_csv("expresstweets.csv")

# create outcome column for sales at EXPRESS
express_outcome <- express_tweets %>%
  unnest_tokens(word, expresstext) %>%
  group_by(word) %>%
  mutate (sale = ifelse(word %in% sale_words, 1, 0)) %>%
  filter(sale == 1)

# join with weather, stock, unemployment, data
express <- left_join(weather_unemp, express_outcome, by = "date")
express <- full_join(express, exp_stocks, by = "date")

# create 0 in outcome column for sales
express <- express %>% 
  mutate(sale = ifelse(is.na(sale),0,sale)) %>%
  select(-word) %>%
  unique()

# create time from last sale variable
express_hist <- express %>% filter(date > "2014-07-10")
setDT(express_hist)[,tte := seq.int(0,.N-1L), by = cumsum(sale)-sale]

###

# GAP
# get tweets
gap_tweets <- read_csv("gaptweets.csv")

# create outcome column for sales at GAP
gap_outcome <- gap_tweets %>%
  unnest_tokens(word, gaptext) %>%
  group_by(word) %>%
  mutate (sale = ifelse(word %in% sale_words, 1, 0)) %>%
  filter(sale == 1)

# join with weather, stock, unemployment, data
gap <- left_join(weather_unemp, gap_outcome, by = "date")
gap <- full_join(gap, gap_stocks, by = "date")

# create 0 in outcome column for sales
gap <- gap %>% 
  mutate(sale = ifelse(is.na(sale),0,sale)) %>%
  select(-word) %>%
  unique()

# create time from last sale variable
gap_hist <- gap %>% filter(date > "2015-02-12")
setDT(gap_hist)[,tte := seq.int(0,.N-1L), by = cumsum(sale)-sale]

###

# JCREW
# get tweets
jcrew_tweets <- read_csv("jcrewtweets.csv")

# create outcome column for sales at JCREW
jcrew_outcome <- jcrew_tweets %>%
  unnest_tokens(word, jcrewtext) %>%
  group_by(word) %>%
  mutate (sale = ifelse(word %in% sale_words, 1, 0)) %>%
  filter(sale == 1)

# join with weather, stock, unemployment, data
jcrew <- left_join(weather_unemp, jcrew_outcome, by = "date")

# create 0 in outcome column for sales
jcrew <- jcrew %>% 
  mutate(sale = ifelse(is.na(sale),0,sale)) %>%
  select(-word) %>%
  unique()

# create time from last sale variable
jcrew_hist <- jcrew %>% filter(date > "2013-09-25")
setDT(jcrew_hist)[,tte := seq.int(0,.N-1L), by = cumsum(sale)-sale]

###

# LOFT
# get tweets
loft_tweets <- read_csv("lofttweets.csv")

# create outcome column for sales at LOFT
loft_outcome <- loft_tweets %>%
  unnest_tokens(word, lofttext) %>%
  group_by(word) %>%
  mutate (sale = ifelse(word %in% sale_words, 1, 0)) %>%
  filter(sale == 1)

# join with weather, stock, unemployment, data
loft <- left_join(weather_unemp, loft_outcome, by = "date")
loft <- full_join(loft, loft_stocks, by = "date")

# create 0 in outcome column for sales
loft <- loft %>% 
  mutate(sale = ifelse(is.na(sale),0,sale)) %>%
  select(-word) %>%
  unique()

# create time from last sale variable
loft_hist <- loft %>% filter(date > "2014-04-20")
setDT(loft_hist)[,tte := seq.int(0,.N-1L), by = cumsum(sale)-sale]



## Create a joined file for visualization
#######################################################################

# EXPRESS
# filter express for only sale dates
express_vis <- express_outcome %>%
  rename(expresssale = sale)

express_vis <- within(express_vis, rm(word))

# GAP
# filter gap for only sale dates
gap_vis <- gap_outcome %>%
  rename(gapsale = sale)

gap_vis <- within(gap_vis, rm(word))

# JCREW
# filter jcrew for only sale dates
jcrew_vis <- jcrew_outcome %>%
  rename(jcrewsale = sale)

jcrew_vis <- within(jcrew_vis, rm(word))

#LOFT
# filter loft for only sale dates
loft_vis <- loft_outcome %>% 
  rename(loftsale = sale)

loft_vis <- within(loft_vis, rm(word))

# join filtered data sets together -- only word & outcome columns
joined <- full_join(express_vis, gap_vis, by = "date")
joined <- full_join(joined, jcrew_vis, by = "date")
joined <- full_join(joined, loft_vis, by = "date")

# join with weather & unemployment
joined <- left_join(weather_unemp, joined, by = "date")

# join with stocks
joined <- full_join(joined, exp_stocks, by = "date")
joined <- full_join(joined, loft_stocks, by = "date")
joined <- full_join(joined, gap_stocks, by = "date")

# replace NAs with 0s
joined <- joined %>% 
  mutate(expresssale = ifelse(is.na(expresssale),0,expresssale)) %>%
  mutate(gapsale = ifelse(is.na(gapsale),0,gapsale)) %>%
  mutate(jcrewsale = ifelse(is.na(jcrewsale),0,jcrewsale)) %>%
  mutate(loftsale = ifelse(is.na(loftsale),0,loftsale))

# save joined dataset for visualizations as a .csv
write.csv(joined, file = "joined_for_visualization.csv", row.names=FALSE)

# remove unneeded weather columns
vis <- joined %>% select(-c(ny_snow, ny_precip, la_snow, la_precip, 
            hou_snow, hou_precip, chi_snow, chi_precip))

# create a mean temperature per day column
vis$mean_temp <- rowMeans(subset(vis, select = c(ny_mean_temp, la_mean_temp, 
                                                 hou_mean_temp, chi_mean_temp)), na.rm = TRUE)

# create a total sales per day column
vis$sum_sales <- rowSums(subset(vis, select = c(expresssale, gapsale, 
                                        loftsale, jcrewsale)), na.rm = TRUE)
```

#2.Visualizations
#Read in data
```{r}
dat<-read_csv("joined_for_visualization.csv")
```

#Plot: Sales over time
The plot for sales over time shows that Express and Loft have more sales per month compared to Gap and JCrew. In addition to having more sales, Express and Loft have greater variation in the number of sales per month. 

```{r}
#Filter data for the correct date range
dat<-dat%>%filter(date>="2015-01-01")

#Create data set with date, store, and sale (Y/N)
dat2<-dat%>%select(date,expresssale,loftsale,jcrewsale,gapsale)
dat2<-gather(dat2,store,sale,2:5)
dat2$store[dat2$store=="expresssale"]  <- "Express"
dat2$store[dat2$store=="loftsale"]  <- "Loft"
dat2$store[dat2$store=="gapsale"]  <- "Gap"
dat2$store[dat2$store=="jcrewsale"]  <- "JCrew"

#Create month/year column and monthly sales column
dat2<-dat2%>%separate(col = `date`, into = c("year", "month", "day"))
dat2<-dat2%>%group_by(month,store)%>%mutate(monthlysales=sum(sale))

#New data frame with one line per month/year for each store
dat3<-dat2%>%select(store,month,monthlysales)
dat3<-unique(dat3)

#Plot all stores on same graph for 2015
p2<-ggplot(dat3, 
           aes(month,monthlysales, color=store))
p2+geom_point()+xlab("Month")+ylab("Number of Sales")+ggtitle("Sales by store in 2015")
```

#Plot: Sales by Temperature & Holiday Over Time

As shown in the below plot, there is no clear relationship between sales, average temperature, and holidays over time. 

```{r}
# visualize weather over time, size = # sales, color = holiday
vis %>% filter(date > "2015-09-25") %>% ggplot(aes(date)) + 
geom_point(aes(y = mean_temp, size = factor(sum_sales), color = factor(holiday))) + 
  geom_line(aes(y = mean_temp))  +
  xlab("Date (By Day)") + ylab("Mean US Temperature") +
  ggtitle("Sales by Temperature & Holiday Over Time")

```

#Plot: Sales by Stock Price Over Time
As shown in the below plots, across stores, there is no clear relationship between stock price and sales over time. 
```{r}
# VISUALIZATION with STOCKS
# visualize stock price over time, color denotes sale day
# LOFT
vis %>% filter(date > "2015-04-20") %>%
  ggplot(aes(date)) + 
  geom_point(aes(y = loft_adj_close, color = factor(loftsale))) +
  geom_line(aes(y = loft_adj_close)) +
  xlab("Date (By Day)") + ylab("Loft Stock Price (ASNA)") +
  ggtitle("Loft Sales by Stock Price Over Time")

# EXPRESS
vis %>% filter(date > "2015-04-20") %>%
  ggplot(aes(date)) + 
  geom_point(aes(y = expr_adj_close, color = factor(expresssale))) +
  geom_line(aes(y = expr_adj_close)) +
  xlab("Date (By Day)") + ylab("Express Stock Price (EXPR)") +
  ggtitle("Express Sales by Stock Price Over Time")

# GAP
vis %>% filter(date > "2015-04-20") %>%
  ggplot(aes(date)) + 
  geom_point(aes(y = gap_adj_close, color = factor(gapsale))) +
  geom_line(aes(y = gap_adj_close)) +
  xlab("Date (By Day)") + ylab("GAP Stock Price (GPS)") +
  ggtitle("Gap Sales by Stock Price Over Time")

```

#Plot: Histograms for Time From Last Sale
The histograms below show the frequency of sales across stores. Among the four stores, Express has sales most frequently, while JCrew has sales least frequently.

```{r}

# LOFT
# visualize time from last sale
hist(loft_hist$tte, 
     main="Histogram for Time From Last Loft Sale", 
     xlab="Time Since Last Sale", 
     col="light blue")

# GAP
# visualize time from last sale
hist(gap_hist$tte, 
     main="Histogram for Time From Last Gap Sale", 
     xlab="Time Since Last Sale", 
     col="blue")

# JCREW
# visualize time from last sale
hist(jcrew_hist$tte, 
     main="Histogram for Time From Last JCrew Sale", 
     xlab="Time Since Last Sale", 
     col="purple")

# EXPRESS
# visualize time from last sale
hist(express_hist$tte,
          main="Histogram for Time From Last Express Sale", 
          xlab="Time Since Last Sale", 
          col="light green")

```

#3. Predictions
  
#EXPRESS
```{r}
#filter for date of furthest tweet back
express <- express %>% filter(date >= "2014-07-10" & date <= "2016-04-20")

require(data.table)

# create a time to event variable
setDT(express)[,tte := seq.int(0,.N-1L), by = cumsum(sale)-sale]

#split data
inTrain <- createDataPartition(y = express$sale, p=0.7)$Resample

#define train and test set
train_set_express <- slice(express, inTrain) 
test_set_express <- slice(express, -inTrain) 

#fit OLS regression to train set
express_ols <- train_set_express %>% lm(sale ~ ny_precip + ny_mean_temp + la_precip + la_mean_temp +  chi_precip + chi_mean_temp + hou_precip + hou_mean_temp + unemployment + holiday + expr_adj_close + tte, data = .)

summary(express_ols)
                                          
#fit glm logistic regression to train set
express_glm <- train_set_express %>% glm(sale ~ ny_precip + ny_mean_temp + la_precip + la_mean_temp +  chi_precip + chi_mean_temp + hou_precip + hou_mean_temp + unemployment + holiday + expr_adj_close + tte, data=. , family = "binomial")

summary(express_glm)
                         
#try ols on test set to check accuracy
test_set_express <- mutate(test_set_express, fhat_express_ols = predict(express_ols, newdata = test_set_express, type = "response")) %>%
  mutate(pred_express_ols=round(fhat_express_ols)) %>% mutate(accuracy_express_ols = (ifelse(pred_express_ols == sale, 1, 0))) 

table(test_set_express$accuracy_express_ols)

#try glm on test set to check accuracy
test_set_express <- mutate(test_set_express, fhat_express_glm = predict(express_glm, newdata = test_set_express, type = "response")) %>%
  mutate(pred_express_glm=round(fhat_express_glm)) %>% mutate(accuracy_express_glm = (ifelse(pred_express_glm == sale, 1, 0))) 

table(prediction = test_set_express$pred_express_glm, truth = test_set_express$sale)
table(test_set_express$sale)

#confusion matrix ols
tab <- table(pred = test_set_express$pred_express_ols, truth = test_set_express$sale)
cm_express_ols <- confusionMatrix(tab)
cm_express_ols$table
cm_express_ols$overall["Accuracy"]

#confusion matrix glm
tab <- table(test_set_express$pred_express_glm, test_set_express$sale)
cm_express_glm <- confusionMatrix(tab)
cm_express_glm $table
cm_express_glm $overall["Accuracy"]
```
Accuracy:
OLS: 71.7%
GLM: 78.1%

Rarely predicting "1" for both GLM and OLS

#GAP
```{r}
#filter for date of furthest tweet back
gap <- gap %>% filter(date >= "2015-02-12" & date <= "2016-04-20")

require(data.table)

# create a time to event variable
setDT(gap)[,tte := seq.int(0,.N-1L), by = cumsum(sale)-sale]

#split data
inTrain <- createDataPartition(y = gap$sale, p=0.7)$Resample

#define train and test set
train_set_gap <- slice(gap, inTrain) 
test_set_gap <- slice(gap, -inTrain) 

#fit OLS regression to train set
gap_ols <- train_set_gap %>% lm(sale ~ ny_precip + ny_mean_temp + la_precip + la_mean_temp +  chi_precip + chi_mean_temp + hou_precip + hou_mean_temp + unemployment + holiday +gap_adj_close +tte, data=.)

summary(gap_ols)

#fit glm logistic regression to train set
gap_glm <- train_set_gap %>% glm(sale ~ ny_precip + ny_mean_temp + la_precip + la_mean_temp +  chi_precip + chi_mean_temp + hou_precip + hou_mean_temp + unemployment + holiday + gap_adj_close + tte, data=. , family = "binomial")

summary(gap_glm)

#try ols on test set to check accuracy
test_set_gap <- mutate(test_set_gap, fhat_gap_ols = predict(gap_ols, newdata = test_set_gap, type = "response")) %>%
  mutate(pred_gap_ols=round(fhat_gap_ols)) %>% mutate(accuracy_gap_ols = (ifelse(pred_gap_ols == sale, 1, 0))) 

table(test_set_gap$accuracy_gap_ols)

#try glm on test set to check accuracy
test_set_gap <- mutate(test_set_gap, fhat_gap_glm = predict(gap_glm, newdata = test_set_gap, type = "response")) %>%
  mutate(pred_gap_glm=round(fhat_gap_glm)) %>% mutate(accuracy_gap_glm = (ifelse(pred_gap_glm == sale, 1, 0))) 

table(test_set_gap$accuracy_gap_glm)

#confusion matrix ols
#tab <- table(test_set_gap$pred_gap_ols, test_set_gap$sale)
#cm_gap_ols <- confusionMatrix(tab)
#cm_gap_ols $table
#cm_gap_ols $overall["Accuracy"]

#confusion matrix glm
#tab <- table(test_set_gap$pred_gap_glm, test_set_gap$sale)
#cm_gap_glm <- confusionMatrix(tab)
#cm_gap_glm$table
#cm_gap_glm$overall["Accuracy"]
```
Always predicting "0" for both OLS and GLM

#LOFT
```{r}
loft <- loft %>% filter(date >= "2014-04-20" & date <= "2016-04-20")

require(data.table)

# create a time to event variable
setDT(loft)[,tte := seq.int(0,.N-1L), by = cumsum(sale)-sale]

#split data
inTrain <- createDataPartition(y = loft$sale, p=0.7)$Resample

#define train and test set
train_set_loft <- slice(loft, inTrain) 
test_set_loft <- slice(loft, -inTrain) 

#fit OLS regression to train set
loft_ols <- train_set_loft %>% lm(sale ~ ny_precip + ny_mean_temp + la_precip + la_mean_temp +  chi_precip + chi_mean_temp + hou_precip + hou_mean_temp + unemployment + holiday + loft_adj_close + tte, data=.)

summary(loft_ols)

#fit glm logistic regression to train set
loft_glm <- train_set_loft %>% glm(sale ~ ny_precip + ny_mean_temp + la_precip + la_mean_temp +  chi_precip + chi_mean_temp + hou_precip + hou_mean_temp + unemployment + holiday + loft_adj_close + tte, data=. , family = "binomial")

summary(loft_glm)

#try ols on test set to check accuracy
test_set_loft <- mutate(test_set_loft, fhat_loft_ols = predict(loft_ols, newdata = test_set_loft, type = "response")) %>%
  mutate(pred_loft_ols=round(fhat_loft_ols)) %>% mutate(accuracy_loft_ols = (ifelse(pred_loft_ols == sale, 1, 0))) 

table(test_set_loft$accuracy_loft_ols)

#try glm on test set to check accuracy
test_set_loft <- mutate(test_set_loft, fhat_loft_glm = predict(loft_glm, newdata = test_set_loft, type = "response")) %>%
  mutate(pred_loft_glm=round(fhat_loft_glm)) %>% mutate(accuracy_loft_glm = (ifelse(pred_loft_glm == sale, 1, 0))) 

table(test_set_loft$accuracy_loft_glm)

#confusion matrix ols
#tab <- table(test_set_loft$pred_loft_ols, test_set_loft$sale)
#cm_loft_ols <- confusionMatrix(tab)
#cm_loft_ols $table
#cm_loft_ols $overall["Accuracy"]

#confusion matrix glm
#tab <- table(test_set_loft$pred_loft_glm, test_set_loft$sale)
#cm_loft_glm <- confusionMatrix(tab)
#cm_loft_glm$table
#cm_loft_glm$overall["Accuracy"]
```
Always predicting "0" for both OLS and GLM

#JCREW
```{r}
jcrew <- jcrew %>% filter(date >= "2013-09-25"& date <= "2016-04-20")

# create a time to event variable
setDT(jcrew)[,tte := seq.int(0,.N-1L), by = cumsum(sale)-sale]

#split data
inTrain <- createDataPartition(y = jcrew$sale, p=0.7)$Resample

#define train and test set
train_set_jcrew <- slice(jcrew, inTrain) 
test_set_jcrew <- slice(jcrew, -inTrain) 

#fit OLS regression to train set
jcrew_ols <- train_set_jcrew %>% lm(sale ~ ny_precip + ny_mean_temp + la_precip + la_mean_temp +  chi_precip + chi_mean_temp + hou_precip + hou_mean_temp + unemployment + holiday +tte, data=.)

summary(jcrew_ols)

#fit glm logistic regression to train set
jcrew_glm <- train_set_jcrew %>% glm(sale ~ ny_precip + ny_mean_temp + la_precip + la_mean_temp +  chi_precip + chi_mean_temp + hou_precip + hou_mean_temp + unemployment + holiday, data=. , family = "binomial")

summary(jcrew_glm)

#try ols on test set to check accuracy
test_set_jcrew <- mutate(test_set_jcrew, fhat_jcrew_ols = predict(jcrew_ols, newdata = test_set_jcrew, type = "response")) %>%
  mutate(pred_jcrew_ols=round(fhat_jcrew_ols)) %>% mutate(accuracy_jcrew_ols = (ifelse(pred_jcrew_ols == sale, 1, 0))) 

table(test_set_jcrew$accuracy_jcrew_ols)

#try glm on test set to check accuracy
test_set_jcrew <- mutate(test_set_jcrew, fhat_jcrew_glm = predict(jcrew_glm, newdata = test_set_jcrew, type = "response")) %>%
  mutate(pred_jcrew_glm=round(fhat_jcrew_glm)) %>% mutate(accuracy_jcrew_glm = (ifelse(pred_jcrew_glm == sale, 1, 0))) 

table(test_set_jcrew$accuracy_jcrew_glm)

#confusion matrix ols
#tab <- table(test_set_jcrew$pred_jcrew_ols, test_set_jcrew$sale)
#cm_jcrew_ols <- confusionMatrix(tab)
#cm_jcrew_ols $table
#cm_jcrew_ols $overall["Accuracy"]

#confusion matrix glm
#tab <- table(test_set_jcrew$pred_jcrew_glm, test_set_jcrew$sale)
#cm_jcrew_glm <- confusionMatrix(tab)
#cm_jcrew_glm$table
#cm_jcrew_glm$overall["Accuracy"]
```
Always predicting "0" for both OLS and GLM


#Discussion
Both our OLS and our logistic regression models almost always predict that there is less than a 50% chance of a sale. This means that we can accurately predict when there is no sale, but our models aren’t meaningful for predicting when sales will happen. As a result, accuracy metrics do not fully communicate the appropriateness of our models for predicting the outcome of interest. In other words, accuracy metrics mainly reflect our ability to predict when there is no sale because we are almost always predicting a low chance of having a sale. This is because our predictor variables jointly have very limited predictive power on our outcome of interest “sale”. 

Overall, these results likely point to the fact that factors outside of the predictors we have included in our model are those that are more strongly predictive of sales. It is also possible that the factors we included in our models impact sales with  time lag, which is not captured in the simple linear models that we have used. Given the data and methods we used, we were unable to find significant associations between “conventional wisdom” factors and retail sales at four stores: Express, LOFT, Gap, and J Crew. Some next steps we could take would be to consider other factors as predictors and to work with time lag models for longitudinal data to capture possible delayed reactions in the data. 

