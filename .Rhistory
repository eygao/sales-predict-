<<<<<<< HEAD
dat27 <- mutate(dat27, label =  as.character(label))
tmp <- lapply( c(40,45), function(i){
expand.grid(Row=1:28, Column=1:28) %>%
mutate(id=i, label=dat27$label[i],
value = unlist(dat27[i,-1]))
})
tmp <- Reduce(rbind, tmp)
tmp %>% ggplot(aes(Row, Column, fill=value)) +
geom_raster() +
scale_y_reverse() +
scale_fill_gradient(low="white", high="black") +
facet_grid(.~label) +
geom_vline(xintercept = 14.5) +
geom_hline(yintercept = 14.5)
row_column <- expand.grid(row=1:28, col=1:28)
ind1 <- which(row_column$col <= 14 & row_column$row <=14)
ind2 <- which(row_column$col > 14 & row_column$row > 14)
ind <- c(ind1,ind2)
X <- as.matrix(dat27[,-1])
X <- X>200
X1 <- rowSums(X[,ind1])/rowSums(X)
X2 <- rowSums(X[,ind2])/rowSums(X)
dat27 <- mutate(dat27, X_1 = X1, X_2 = X2)
y <- as.factor(dat27$label)
x <- cbind(X1, X2)
library(RColorBrewer)
my_colors <- brewer.pal(11,"RdBu")
library(caret)
fit <- knn3(x, y, 401)
GS <- 150
X1s <- seq(min(X1),max(X1),len=GS)
X2s <- seq(min(X2),max(X2),len=GS)
df <- expand.grid(X_1=X1s, X_2=X2s)
yhat <- predict(fit, newdata = df, type="prob")[,2]
df <- mutate(df, yhat=yhat)
f <- loess(yhat~X_1*X_2, data=df,
degree=1, span=1/5)$fitted
df <- df %>% mutate(f=f)
true_f <- df %>%
ggplot(aes(X_1, X_2, fill=f))  +
scale_fill_gradientn(colors=c("#00BFC4","white","#F8766D"))+ geom_raster()
true_f
library(caret)
inTrain <- createDataPartition(y = dat$label, p=0.5)
train_set <- slice(dat, inTrain$Resample1)
test_set <- slice(dat, -inTrain$Resample1)
inTrain <- createDataPartition(y = dat$label, p=0.5)
library(caret)
inTrain <- createDataPartition(y = dat$label, p=0.5)
set.seed(1971)
dat <- sample_n(dat27, 1000)
dat <- dat %>% mutate(y = ifelse(label=="2",0,1 ))
library(caret)
inTrain <- createDataPartition(y = dat$label, p=0.5)
train_set <- slice(dat, inTrain$Resample1)
test_set <- slice(dat, -inTrain$Resample1)
geom_point(pch=21,cex=2)
train_set %>% ggplot(aes(X_1, X_2, fill = label)) +
geom_point(pch=21,cex=2)
fit <-  glm(y~X_1+X_2, data=train_set, family="binomial")
fit
GS <- 150
for_plot <- expand.grid(
X_1 = seq(min(train_set$X_1),max(train_set$X_1),len=GS),
X_2 = seq(min(train_set$X_2),max(train_set$X_2),len=GS))
f_hat <- predict(fit, newdata=for_plot, type="response")
g1 <- for_plot %>% mutate(f_hat = f_hat) %>%
ggplot(aes(X_1, X_2, fill=f_hat))  +
scale_fill_gradientn(colors=c("#00BFC4","white","#F8766D"))+ geom_raster()
library(gridExtra)
grid.arrange(true_f, g1, nrow=1)
library(dplyr)
library(ggplot2)
theme_set(theme_bw(base_size = 16))
library(gganimate)
library(readr)
library(knitr)
library(broom)
library(stringr)
library(lubridate)
library(tidyr)
library(XML)
theurl <- paste0("http://www.pollster.com/08USPresGEMvO-2.html")
polls_2008 <- readHTMLTable(theurl,stringsAsFactors=FALSE)[[1]] %>%
tbl_df() %>%
separate(col=Dates, into=c("start_date","end_date"), sep="-",fill="right") %>%
mutate(end_date = ifelse(is.na(end_date), start_date, end_date)) %>%
separate(start_date, c("smonth", "sday", "syear"), sep = "/",  convert = TRUE, fill = "right")%>%
mutate(end_date = ifelse(str_count(end_date, "/") == 1, paste(smonth, end_date, sep = "/"), end_date)) %>%
mutate(end_date = mdy(end_date))  %>% mutate(syear = ifelse(is.na(syear), year(end_date), syear + 2000)) %>%
unite(start_date, smonth, sday, syear)  %>%
mutate(start_date = mdy(start_date)) %>%
separate(`N/Pop`, into=c("N","population_type"), sep="\ ", convert=TRUE, fill="left") %>%
mutate(Obama = as.numeric(Obama)/100,
McCain=as.numeric(McCain)/100,
diff = Obama - McCain,
day=as.numeric(start_date - mdy("11/04/2008")))
polls_2008
dat <-  filter(polls_2008, start_date>="2008-06-01") %>%
group_by(X=day)  %>%
summarize(Y=mean(diff))
dat %>% ggplot(aes(X, Y)) + geom_point() + geom_smooth(method = "lm", se = FALSE)
resids <- ifelse(lm(Y~X, data=dat)$resid >0, "+", "-")
dat %>% mutate(resids=resids) %>%
ggplot(aes(X, Y)) +
geom_point(cex=5,pch=21) +
geom_smooth(method = "lm", se = FALSE) +
geom_point(aes(X,Y,color=resids), cex=4)
span <- 7
dat2 <- dat %>%
inflate(center = unique(dat$X)) %>%
mutate(dist = abs(X - center)) %>%
filter(dist <= span) %>%
mutate(weight =  1)
dat2 %>% filter(center %in% c(-125, -55)) %>%
ggplot(aes(X, Y)) +
geom_point(aes(alpha = weight)) +
geom_smooth(aes(group = center, frame = center, weight = weight),
method = "lm", formula=y~1, se = FALSE) +
geom_vline(aes(xintercept = center, frame = center), lty = 2) +
geom_point(shape = 1, data = dat)
mod <- ksmooth(dat$X, dat$Y, kernel="box", bandwidth = span)
bin_fit <- data.frame(X=dat$X, .fitted=mod$y)
p <- ggplot(dat2, aes(X, Y)) +
geom_point(aes(alpha = weight, frame = center),cex=5) +
geom_smooth(aes(group = center, frame = center, weight = weight),
method = "lm", formula=y~1, se = FALSE) +
geom_vline(aes(xintercept = center, frame = center), lty = 2) +
geom_point(shape = 1, data = dat, alpha = .5,cex=5) +
geom_line(aes(x=X, y = .fitted, frame = X, cumulative = TRUE), data = bin_fit, color = "red") + ggtitle("x0 = ")
gg_animate(p, "binsmoother1.gif", ani.width = 1050, ani.height = 525, interval=0.15)
mod <- ksmooth(dat$X, dat$Y, kernel="box", bandwidth = span)
bin_fit <- data.frame(X=dat$X, .fitted=mod$y)
ggplot(dat, aes(X, Y)) +
geom_point(cex=5) + geom_line(aes(x=X, y=.fitted),
data=bin_fit, color="red")
dat2 <- dat %>%
inflate(center = unique(dat$X)) %>%
mutate(dist = abs(X - center)) %>%
filter(dist <= span) %>%
mutate(weight =  dnorm(dist,0,span/2.54))%>%
mutate(weight = weight/max(weight))
mod <- ksmooth(dat$X, dat$Y, kernel="normal", bandwidth = span)
bin_fit2 <- data.frame(X=dat$X, .fitted=mod$y)
p <- ggplot(dat2, aes(X, Y)) +
geom_point(aes(alpha = weight, frame = center),cex=5) +
geom_smooth(aes(group = center, frame = center, weight = weight),
method = "lm", formula=y~1, se = FALSE) +
geom_vline(aes(xintercept = center, frame = center), lty = 2) +
geom_point(shape = 1, data = dat, alpha = .5,cex=5) +
geom_line(aes(x=X, y = .fitted, frame = X, cumulative = TRUE),data = bin_fit2,color ="red") +
ggtitle("x0 = ")
gg_animate(p, "binsmoother2.gif", ani.width = 1050, ani.height = 525, interval=0.15)
mod <- ksmooth(dat$X, dat$Y, kernel="normal",
bandwidth = span)
bin_fit2 <- data.frame(X=dat$X, .fitted=mod$y)
ggplot(dat, aes(X, Y)) +
geom_point(cex=5) + geom_line(aes(x=X, y=.fitted), data=bin_fit2, color="red")
span <- 0.05
dat2 <- dat %>%
inflate(center = unique(dat$X)) %>%
mutate(dist = abs(X - center)) %>%
filter(rank(dist) / n() <= span) %>%
mutate(weight = (1 - (dist / max(dist)) ^ 3) ^ 3)
dat2 %>% filter(center %in% c(-125, -55)) %>%
ggplot(aes(X, Y)) +
geom_point(aes(alpha = weight)) +
geom_smooth(aes(group = center, frame = center, weight = weight),
method = "lm", se = FALSE) +
geom_vline(aes(xintercept = center, frame = center), lty = 2) +
geom_point(shape = 1, data = dat)
span <- 0.15
dat2 <- dat %>%
inflate(center = unique(dat$X)) %>%
mutate(dist = abs(X - center)) %>%
filter(rank(dist) / n() <= span) %>%
mutate(weight = (1 - (dist / max(dist)) ^ 3) ^ 3)
dat2 %>% filter(center %in% c(-125, -55)) %>%
ggplot(aes(X, Y)) +
geom_point(aes(alpha = weight), cex=5) +
geom_smooth(aes(group = center, frame = center, weight = weight),
method = "lm", se = FALSE) +
geom_vline(aes(xintercept = center, frame = center), lty = 2) +
geom_point(shape = 1, data = dat,cex=5)
mod <- loess(Y~X, degree=1, span = span, data=dat)
loess_fit <- augment(mod)
p <- ggplot(dat2, aes(X, Y)) +
geom_point(aes(alpha = weight, frame = center),cex=5) +
geom_smooth(aes(group = center, frame = center, weight = weight), method = "lm", se = FALSE) +
geom_vline(aes(xintercept = center, frame = center), lty = 2) +
geom_point(shape = 1, data = dat, alpha = .5,cex=5) +
geom_line(aes(x=X, y = .fitted, frame = X, cumulative = TRUE), data = loess_fit, color = "red",lwd=1.5) +
ggtitle("x0 = ")
gg_animate(p, "loess.gif", ani.width = 1050, ani.height = 525, interval=0.15)
mod <- loess(Y~X, degree=1, span = span, data=dat)
loess_fit <- augment(mod)
p <- ggplot(dat2, aes(X, Y)) +
geom_point(aes(alpha = weight, frame = center),cex=5) +
geom_smooth(aes(group = center, frame = center, weight = weight), method = "lm", se = FALSE) +
geom_vline(aes(xintercept = center, frame = center), lty = 2) +
geom_point(shape = 1, data = dat, alpha = .5,cex=5) +
geom_line(aes(x=X, y = .fitted, frame = X, cumulative = TRUE), data = loess_fit, color = "red",lwd=1.5) +
ggtitle("x0 = ")
mod <- loess(Y~X, degree=1, span = span, data=dat)
loess_fit <- augment(mod)
ggplot(dat, aes(X, Y)) +
geom_point(cex=5) + geom_line(aes(x=X, y=.fitted), data=loess_fit, color="red")
library(rafalib)
mypar()
plot(c(0,1,1),c(0,0,1),pch=16,cex=2,xaxt="n",yaxt="n",xlab="",ylab="",bty="n",xlim=c(-0.25,1.25),ylim=c(-0.25,1.25))
lines(c(0,1,1,0),c(0,0,1,0))
text(0,.2,expression(paste('(A'[x]*',A'[y]*')')),cex=1.5)
text(1,1.2,expression(paste('(B'[x]*',B'[y]*')')),cex=1.5)
text(-0.1,0,"A",cex=2)
text(1.1,1,"B",cex=2)
url <- "https://raw.githubusercontent.com/datasciencelabs/data/master/hand-written-digits-train.csv"
digits <- read_csv(url)
dat27 <- digits %>% filter(label%in%c(2,7))
dat27 <- mutate(dat27, label =  as.character(label)) %>%
mutate(y = ifelse(label=="2",0,1 ))
row_column <- expand.grid(row=1:28, col=1:28)
ind1 <- which(row_column$col <= 14 & row_column$row <=14)
ind2 <- which(row_column$col > 14 & row_column$row > 14)
ind <- c(ind1,ind2)
X <- as.matrix(dat27[,-1])
X <- X>200
X1 <- rowSums(X[,ind1])/rowSums(X)
X2 <- rowSums(X[,ind2])/rowSums(X)
dat27 <- mutate(dat27, X_1 = X1, X_2 = X2)
y <- as.factor(dat27$label)
x <- cbind(X1, X2)
library(caret)
fit <- knn3(x, y, 401)
GS <- 150
X1s <- seq(min(X1),max(X1),len=GS)
X2s <- seq(min(X2),max(X2),len=GS)
true_f <- expand.grid(X_1=X1s, X_2=X2s)
yhat <- predict(fit, newdata = true_f, type="prob")[,2]
true_f <- mutate(true_f, yhat=yhat)
f <- loess(yhat~X_1*X_2, data=true_f,
degree=1, span=1/5)$fitted
true_f <- true_f %>% mutate(f=f)
##create the training set
set.seed(1)
dat <- sample_n(dat27, 1000)
library(caret)
inTrain <- createDataPartition(y = dat$label,
p=0.5)
train_set <- slice(dat, inTrain$Resample1)
test_set <- slice(dat, -inTrain$Resample1)
knn_fit <- knn3(y~.,data = select(train_set, y, X_1, X_2) )
f_hat <- predict(knn_fit, newdata = test_set)[,2]
tab <- table(pred=round(f_hat), truth=test_set$y)
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
f_hat <- predict(knn_fit, newdata = true_f)[,2]
g1 <- true_f %>% mutate(f_hat = f_hat) %>%
ggplot(aes(X_1, X_2, fill=f_hat))  +
scale_fill_gradientn(colors=c("#00BFC4","white","#F8766D")) + geom_raster()  + guides(fill=FALSE) +
stat_contour(aes(x=X_1,y=X_2,z=f_hat),data=true_f, breaks=c(0.5),color="black",lwd=1.5)
g2 <- ggplot(true_f) +
geom_point(data=train_set, aes(x=X_1, y=X_2, fill=label), cex=5, pch=21) + stat_contour(aes(X_1,X_2,z=f_hat), breaks=c(0.5),color="black",lwd=1.5)
library(gridExtra)
grid.arrange(g1,g2, nrow=1)
control <- trainControl(method='cv', number=2, p=.5)
dat2 <- mutate(dat, label=as.factor(label)) %>%
select(label,X_1,X_2)
res <- train(label ~ .,
data = dat2,
method = "knn",
trControl = control,
tuneLength = 1, # How fine a mesh to go on grid
tuneGrid=data.frame(k=seq(3,151,2)),
metric="Accuracy")
plot(res)
library(dplyr)
library(ggplot2)
theme_set(theme_bw(base_size = 16))
library(gganimate)
library(readr)
library(knitr)
library(broom)
library(gridExtra)
url <- "https://raw.githubusercontent.com/datasciencelabs/data/master/hand-written-digits-train.csv"
if(!exists("digits")) digits <- read_csv(url)
dat <- digits %>% filter(label%in%c(2,7))
tmp <- lapply( c(37,9,5,28), function(i){
expand.grid(Row=1:28, Column=1:28) %>%
mutate(id=i, label=dat$label[i], id=paste("obs",i),
value = unlist(select(dat,pixel0:pixel783)[i,]))
})
tmp <- Reduce(rbind, tmp)
tmp %>% ggplot(aes(Row, Column, fill=value)) +
geom_raster() +
scale_y_reverse() +
scale_fill_gradient(low="white", high="black") +
geom_vline(xintercept = 14.5) +
geom_hline(yintercept = 14.5)  +
facet_grid(.~id)
dat <- mutate(dat, label =  as.character(label)) %>%
mutate(y = ifelse(label=="2",0,1 ))
row_column <- expand.grid(row=1:28, col=1:28)
ind1 <- which(row_column$col <= 14 & row_column$row <=14)
ind2 <- which(row_column$col > 14 & row_column$row > 14)
ind <- c(ind1,ind2)
X <- as.matrix(dat[,-1])
X <- X>200
X1 <- rowSums(X[,ind1])/rowSums(X)
X2 <- rowSums(X[,ind2])/rowSums(X)
dat <- mutate(dat, X_1 = X1, X_2 = X2)
y <- as.factor(dat$label)
x <- cbind(X1, X2)
library(caret)
fit <- knn3(x, y, 51)
GS <- 150
X1s <- seq(min(X1),max(X1),len=GS)
X2s <- seq(min(X2),max(X2),len=GS)
true_f <- expand.grid(X_1=X1s, X_2=X2s)
yhat <- predict(fit, newdata = true_f, type="prob")[,2]
true_f <- mutate(true_f, yhat=yhat)
f <- loess(yhat~X_1*X_2, data=true_f,
degree=1, span=1/5)$fitted
true_f <- true_f %>% mutate(f=f)
rm(X,X1,X2,fit,GS,X1s,X2s,yhat,f)
true_f_plot <- true_f %>%
ggplot(aes(X_1, X_2, fill=f))  +
scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + geom_raster()  + #guides(fill=FALSE) +
stat_contour(aes(x=X_1,y=X_2,z=f), data=true_f, breaks=c(0.5),color="black",lwd=1.5)
true_f_plot
set.seed(1)
dat <- sample_n(dat, 1000) %>% select(y, X_1, X_2)
dat
options(digits = 2)
params <- train_set %>% group_by(y) %>%
summarize(avg_1 = mean(X_1), avg_2 = mean(X_2), sd_1= sd(X_1), sd_2 = sd(X_2), r = cor(X_1,X_2))
params
library(caret)
inTrain <- createDataPartition(y = dat$y, p=0.5)
train_set <- slice(dat, inTrain$Resample1)
test_set <- slice(dat, -inTrain$Resample1)
options(digits = 2)
params <- train_set %>% group_by(y) %>%
summarize(avg_1 = mean(X_1), avg_2 = mean(X_2), sd_1= sd(X_1), sd_2 = sd(X_2), r = cor(X_1,X_2))
params
train_set %>% mutate(y = factor(y)) %>%
ggplot(aes(X_1, X_2, fill = y, color=y)) +
geom_point(pch=21,cex=5, color="black") +
stat_ellipse(lwd=2, type="norm")
library(mvtnorm)
get_p <- function(params, data){
dmvnorm( cbind(data$X_1, data$X_2),
mean = c(params$avg_1, params$avg_2),
sigma = matrix( c(params$sd_1^2,
params$sd_1*params$sd_2*params$r,
params$sd_1*params$sd_2*params$r,
params$sd_2^2),2,2))
}
pi <- 0.5
p0 <- get_p(params[1,], true_f)
p1 <- get_p(params[2,], true_f)
f_hat_qda <- pi*p1/(pi*p1 + (1-pi)*p0)
p <-true_f %>% mutate(f=f_hat_qda) %>%
ggplot(aes(X_1, X_2, fill=f))  +
scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
geom_raster()  + #guides(fill=FALSE) +
stat_contour(aes(x=X_1,y=X_2,z=f),
data=mutate(true_f, f=f_hat_qda),
breaks=c(0.5),color="black",lwd=1.5)
grid.arrange(true_f_plot, p, nrow=1)
install.packages("mvtnorm")
library(mvtnorm)
get_p <- function(params, data){
dmvnorm( cbind(data$X_1, data$X_2),
mean = c(params$avg_1, params$avg_2),
sigma = matrix( c(params$sd_1^2,
params$sd_1*params$sd_2*params$r,
params$sd_1*params$sd_2*params$r,
params$sd_2^2),2,2))
}
pi <- 0.5
p0 <- get_p(params[1,], true_f)
p1 <- get_p(params[2,], true_f)
f_hat_qda <- pi*p1/(pi*p1 + (1-pi)*p0)
p <-true_f %>% mutate(f=f_hat_qda) %>%
ggplot(aes(X_1, X_2, fill=f))  +
scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
geom_raster()  + #guides(fill=FALSE) +
stat_contour(aes(x=X_1,y=X_2,z=f),
data=mutate(true_f, f=f_hat_qda),
breaks=c(0.5),color="black",lwd=1.5)
grid.arrange(true_f_plot, p, nrow=1)
params <- train_set %>% group_by(y) %>%
summarize(avg_1 = mean(X_1), avg_2 = mean(X_2), sd_1= sd(X_1), sd_2 = sd(X_2), r = cor(X_1,X_2))
params <-params %>% mutate(sd_1 = mean(sd_1), sd_2=mean(sd_1), r=mean(r))
params
library(mvtnorm)
p0 <- get_p(params[1,], data=true_f)
p1 <- get_p(params[2,], data=true_f)
p <- 0.5
f_hat_lda <- pi*p1/(pi*p1 + (1-pi)*p0)
p <- true_f %>% mutate(f=f_hat_lda) %>%
ggplot(aes(X_1, X_2, fill=f))  +
scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
geom_raster()  + #guides(fill=FALSE) +
stat_contour(aes(x=X_1,y=X_2,z=f),
data=mutate(true_f, f=f_hat_lda),
breaks=c(0.5),color="black",lwd=1.5)
grid.arrange(true_f_plot, p, nrow=1)
params <- train_set %>% group_by(y) %>%
summarize(avg_1 = mean(X_1), avg_2 = mean(X_2),
sd_1= sd(X_1), sd_2 = sd(X_2),
r = cor(X_1,X_2))
p0 <- get_p(params[1,], dat= test_set)
p1 <- get_p(params[2,], dat= test_set)
pi <- 0.5
pred_qda <- pi*p1/(pi*p1 + (1-pi)*p0)
test_set %>% mutate(pred=pred_qda, label=as.factor(y)) %>%
ggplot(aes(label,pred)) + geom_boxplot()
install.packages("pROC")
library(pROC)
library(pROC)
roc_qda <- roc(test_set$y, pred_qda)
plot(roc_qda)
params <-params %>% mutate(sd_1 = mean(sd_1),
sd_2 = mean(sd_1),
r=mean(r))
p0 <- get_p(params[1,], dat = test_set)
p1 <- get_p(params[2,], dat = test_set)
pi <- 0.5
pred_lda <- pi*p1/(pi*p1 + (1-pi)*p0)
roc_lda <- roc(test_set$y, pred_lda)
plot(roc_qda)
plot(roc_lda, add=TRUE, col=2)
fit <- knn3(y~., data = train_set, k=5)
pred_knn_5 <- predict(fit, newdata = test_set)[,2]
roc_knn_5 <- roc(test_set$y, pred_knn_5)
plot(roc_qda)
plot(roc_knn_5, add=TRUE, col=3)
fit <- knn3(y~., data = train_set, k=51)
pred_knn_51 <- predict(fit, newdata = test_set)[,2]
roc_knn_51 <- roc(test_set$y, pred_knn_51)
plot(roc_qda)
plot(roc_knn_51, add=TRUE, col=4)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
theme_set(theme_bw(base_size = 16))
plotit <- function(dat, i, n=sqrt(ncol(dat)-1)){
dat <- slice(dat,i)
tmp <-  expand.grid(Row=1:n, Column=1:n) %>%
mutate(id=i, label=dat$label,
value = unlist(dat[,-1]))
tmp%>%ggplot(aes(Row, Column, fill=value)) +
geom_raster() +
scale_y_reverse() +
scale_fill_gradient(low="white", high="black") +
ggtitle(tmp$label[1])
}
url <- "https://raw.githubusercontent.com/datasciencelabs/data/master/hand-written-digits-train.csv"
original_dat <- read_csv(url)
original_dat <- mutate(original_dat, label = as.factor(label))
url <- "https://raw.githubusercontent.com/datasciencelabs/data/master/hand-written-digits-test.csv"
original_test<- read_csv(url)
View(original_test)
X <- sample_n(original_dat,200) %>%
arrange(label)
d <- dist(as.matrix(X[,-1]))
image(as.matrix(d))
plot(hclust(d),labels=as.character(X$label))
```{r options, echo=FALSE}
library(knitr)
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
library(dplyr)
library(ggplot2)
theme_set(theme_bw(base_size = 16))
library(rafalib)
set.seed(1)
n <- 100
lim <- c(60,78)
X <- MASS::mvrnorm(n,
c(69,69),
matrix(c(9,9*0.92,9*0.92,9*1),2,2))
mypar(1,1)
plot(X, xlim=lim, ylim=lim)
points(X[1:2,], col="red", pch=16)
lines(X[1:2,],col="red")
install.packages("rafalib")
library(rafalib)
set.seed(1)
n <- 100
lim <- c(60,78)
X <- MASS::mvrnorm(n,
c(69,69),
matrix(c(9,9*0.92,9*0.92,9*1),2,2))
mypar(1,1)
plot(X, xlim=lim, ylim=lim)
points(X[1:2,], col="red", pch=16)
lines(X[1:2,],col="red")
library("shiny")
runExample("01_hello")
library(ROAuth)
library(streamR)
setwd("~/Dropbox (Personal)/Spring 2015/Data Science/sales-predict-")
library(twitteR)
consumerKey = "0c9bYRN6ZIVGjBVQQB9eXuzuQ"   # from your app name
consumerSecret = "SImuevFXUWSU6RtwUtl0qwTwfdGNJCPO1dzJl85TQglBHz45YS"
accessToken = "385217925-0zxobdY5cZoLOe5JpyP8H6JKd8iMsVdUljF5lMe8"
accessSecret = "vIsWGoKTpB9yvBAyjIONdjZ8NsYVJmfGx2vHZSu9PrXDg"
options(httr_oauth_cache=TRUE) # skip question appearing on console
setup_twitter_oauth(consumer_key = consumerKey, consumer_secret = consumerSecret,
access_token = accessToken, access_secret = accessSecret)
=======
tmp <- data.frame(pca$rotation, name = movie_titles)
tmp %>%  ggplot(aes(PC1, PC2)) + geom_point() +
geom_text_repel(aes(PC1, PC2, label=name),
data = filter(tmp,
PC1 < -0.1 | PC1 >0.1 | PC2 < -0.15 | PC2>0.0))
rmse_results %>% kable
install.packages(shiny)
install.packages("shiny")
require(shiny)
install.packages("shiny")
install.packages("shiny")
require("shiny")
library("shiny")
library(shiny)
server <- function(input, output) {}
shinyApp(ui = ui, server = server)
library(shiny)
# Define UI for application that draws a histogram
ui <- shinyUI(fluidPage(
# Application title
titlePanel("Old Faithful Geyser Data"),
# Sidebar with a slider input for number of bins
sidebarLayout(
sidebarPanel(
sliderInput("bins",
"Number of bins:",
min = 1,
max = 50,
value = 30)
),
# Show a plot of the generated distribution
mainPanel(
plotOutput("distPlot")
)
)
))
# Define server logic required to draw a histogram
server <- shinyServer(function(input, output) {
output$distPlot <- renderPlot({
# generate bins based on input$bins from ui.R
x    <- faithful[, 2]
bins <- seq(min(x), max(x), length.out = input$bins + 1)
# draw the histogram with the specified number of bins
hist(x, breaks = bins, col = 'darkgray', border = 'white')
})
})
# Run the application
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage()
server <- functions(input, output) {}
shinyApp(ui, server)
library(shiny)
ui <- fluidPage()
server <- functions(input, output){}
shinyApp(ui, server)
library(shiny)
ui <- fluidPage()
server <- function(input, output){}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(
sliderInput("num", "Choose a number", 1, 100, 50)
)
server <- function(input, output){}
shinyApp(ui, server)
library(shiny)
ui <- fluidPage(
sliderInput("num", "Choose a number", 1, 100, 50),
plotOutput("hist")
)
server <- function(input, output){}
shinyApp(ui, server)
library(shiny)
ui <- fluidPage(
sliderInput("num", "Choose a number", 1, 100, 50),
plotOutput("hist")
)
server <- function(input, output){
output$hist <-- renderPlot({
hist(iris$Sepal.Length)
})
}
shinyApp(ui, server)
runApp('Shiny_App_Class')
runApp('Shiny_App_Class')
library(shiny)
ui <- fluidPage(
sliderInput("num", "Choose a number", 1, 100, 50),
plotOutput("hist")
)
server <- function(input, output){
output$hist <- renderPlot({
hist(rnorm(input$num))
})
}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(
sliderInput("num", "Choose a number", 1, 100, 50),
plotOutput("hist2")
)
server <- function(input, output){
output$hist2 <- renderPlot({
hist(rnorm(input$num))
})
}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(
sliderInput("num", "Choose a number", 1, 100, 50),
plotOutput("hist2")
)
server <- function(input, output){
output$hist2 <- renderPlot({
hist(rnorm(input$num))
render(verbatimTextOutput(summary(rnorm(input$num))))
})
}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(
sliderInput("num", "Choose a number", 1, 100, 50),
plotOutput("hist2")
)
server <- function(input, output){
output$hist2 <- renderPlot({
hist(rnorm(input$num))
verbatimTextOutput(summary(rnorm(input$num)))
})
}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(
sliderInput("num", "Choose a number", 1, 100, 50),
plotOutput("hist2"),
verbatimTextOutput("sum")
)
server <- function(input, output){
output$hist2 <- renderPlot({
hist(rnorm(input$num))
})
output$sum <- renderPrint({verbatimTextOutput(summary(rnorm(input$num)))
})
}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(
sliderInput("num", "Choose a number", 1, 100, 50),
plotOutput("hist2"),
verbatimTextOutput("sum")
)
server <- function(input, output){
output$hist2 <- renderPlot({
hist(rnorm(input$num))
})
output$sum <- renderPrint({(summary(rnorm(input$num)))
})
}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(
sliderInput("num", "Choose a number", 1, 100, 50),
plotOutput("hist2"),
verbatimTextOutput("sum")
)
server <- function(input, output){
data <- reactive ({
rnorm(input$num)
})
output$hist2 <- renderPlot({
hist(data())
})
output$sum <- renderPrint({
(summary(data())
})
}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(
sliderInput("num", "Choose a number", 1, 100, 50),
plotOutput("hist2"),
verbatimTextOutput("sum")
)
server <- function(input, output){
data <- reactive ({
rnorm(input$num)
})
output$hist2 <- renderPlot({
hist(data())
})
output$sum <- renderPrint({
summary(data())
})
}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(
sliderInput("num", "Choose a number", 1, 100, 50),
plotOutput("hist2"),
verbatimTextOutput("sum")
)
server <- function(input, output){
data <- reactive ({
rnorm(input$num)
})
output$hist2 <- renderPlot({
hist(data())
})
output$sum <- renderPrint({
summary(data())
})
}
shinyApp(ui = ui, server = server)
install.packages("weatherData")
library("weatherData")
dat <- getWeatherForDate("PHNL", "2013-08-10", 2013-08-31")
dat <- getWeatherForDate("PHNL", "2013-08-10", 2013-08-31")
getWeatherForDate()
getWeatherForDate(station_id, start_date, end_date = NULL,
station_type = "airportCode", opt_detailed = FALSE,
opt_write_to_file = FALSE, opt_temperature_columns = TRUE,
opt_all_columns = FALSE, opt_custom_columns = FALSE,
custom_columns = NULL, opt_verbose = FALSE, daily_min = FALSE,
daily_max = FALSE)
dat <- getWeatherForDate("PHNL", "2013-08-10", "2013-08-31")
dat <- getWeatherForDate("PIT", "2013-08-10", "2013-08-31")
getHistoricalWeather <- function(airport.code="SFO", date="Sys.Date()")
{
base.url <- 'http://api.wunderground.com/api/{your key here}/'
# compose final url
final.url <- paste(base.url, 'history_', date, '/q/', airport.code, '.json', sep='')
# reading in as raw lines from the web service
conn <- url(final.url)
raw.data <- readLines(conn, n=-1L, ok=TRUE)
# Convert to a JSON
weather.data <- fromJSON(paste(raw.data, collapse=""))
close(conn)
return(weather.data)
}
# get data for 10 days - restriction by Weather Underground for free usage
date.range <- seq.Date(from=as.Date('2006-1-01'), to=as.Date('2006-1-10'), by='1 day')
# Initialize a data frame
hdwd <- data.frame()
# loop over dates, and fetch weather data
for(i in seq_along(date.range)) {
weather.data <- getHistoricalWeather('SFO', format(date.range[i], "%Y%m%d"))
hdwd <- rbind(hdwd, ldply(weather.data$history$dailysummary,
function(x) c('SJC', date.range[i], x$fog, x$rain, x$snow,  x$meantempi, x$meanvism, x$maxtempi, x$mintempi)))
}
colnames(hdwd) <- c("Airport", "Date", 'Fog', 'Rain', 'Snow','AvgTemp', 'AvgVisibility','MaxTemp','MinTemp')
# save to CSV
write.csv(hdwd, file=gzfile('SFC-Jan2006.csv.gz'), row.names=FALSE)
?fromJSON
??fromJSON
library(RJSONIO)
install.packages("RJSONIO")
library(RJSONIO)
install.packages("plyr")
install.packages("plyr")
library(plyr)
getHistoricalWeather <- function(airport.code="SFO", date="Sys.Date()")
{
base.url <- 'http://api.wunderground.com/api/{your key here}/'
# compose final url
final.url <- paste(base.url, 'history_', date, '/q/', airport.code, '.json', sep='')
# reading in as raw lines from the web service
conn <- url(final.url)
raw.data <- readLines(conn, n=-1L, ok=TRUE)
# Convert to a JSON
weather.data <- fromJSON(paste(raw.data, collapse=""))
close(conn)
return(weather.data)
}
# get data for 10 days - restriction by Weather Underground for free usage
date.range <- seq.Date(from=as.Date('2006-1-01'), to=as.Date('2006-1-10'), by='1 day')
# Initialize a data frame
hdwd <- data.frame()
# loop over dates, and fetch weather data
for(i in seq_along(date.range)) {
weather.data <- getHistoricalWeather('SFO', format(date.range[i], "%Y%m%d"))
hdwd <- rbind(hdwd, ldply(weather.data$history$dailysummary,
function(x) c('SJC', date.range[i], x$fog, x$rain, x$snow,  x$meantempi, x$meanvism, x$maxtempi, x$mintempi)))
}
colnames(hdwd) <- c("Airport", "Date", 'Fog', 'Rain', 'Snow','AvgTemp', 'AvgVisibility','MaxTemp','MinTemp')
# save to CSV
write.csv(hdwd, file=gzfile('SFC-Jan2006.csv.gz'), row.names=FALSE)
getHistoricalWeather <- function(airport.code="SFO", date="Sys.Date()")
{
base.url <- 'http://api.wunderground.com/api/{your key here}/'
# compose final url
final.url <- paste(base.url, 'history_', date, '/q/', airport.code, '.json', sep='')
# reading in as raw lines from the web service
conn <- url(final.url)
raw.data <- readLines(conn, n=-1L, ok=TRUE)
# Convert to a JSON
weather.data <- fromJSON(paste(raw.data, collapse=""))
close(conn)
return(weather.data)
}
# get data for 10 days - restriction by Weather Underground for free usage
date.range <- seq.Date(from=as.Date('2006-1-01'), to=as.Date('2006-1-10'), by='1 day')
# Initialize a data frame
hdwd <- data.frame()
# loop over dates, and fetch weather data
for(i in seq_along(date.range)) {
weather.data <- getHistoricalWeather('SFO', format(date.range[i], "%Y%m%d"))
hdwd <- rbind(hdwd, ldply(weather.data$history$dailysummary,
function(x) c('SJC', date.range[i], x$fog, x$rain, x$snow,  x$meantempi, x$meanvism, x$maxtempi, x$mintempi)))
}
colnames(hdwd) <- c("Airport", "Date", 'Fog', 'Rain', 'Snow','AvgTemp', 'AvgVisibility','MaxTemp','MinTemp')
# save to CSV
write.csv(hdwd, file=gzfile('SFC-Jan2006.csv.gz'), row.names=FALSE)
getHistoricalWeather <- function(airport.code="SFO", date="Sys.Date()")
{
base.url <- 'http://api.wunderground.com/api/HSPHDatScience/'
# compose final url
final.url <- paste(base.url, 'history_', date, '/q/', airport.code, '.json', sep='')
# reading in as raw lines from the web service
conn <- url(final.url)
raw.data <- readLines(conn, n=-1L, ok=TRUE)
# Convert to a JSON
weather.data <- fromJSON(paste(raw.data, collapse=""))
close(conn)
return(weather.data)
}
# get data for 10 days - restriction by Weather Underground for free usage
date.range <- seq.Date(from=as.Date('2006-1-01'), to=as.Date('2006-1-10'), by='1 day')
# Initialize a data frame
hdwd <- data.frame()
# loop over dates, and fetch weather data
for(i in seq_along(date.range)) {
weather.data <- getHistoricalWeather('SFO', format(date.range[i], "%Y%m%d"))
hdwd <- rbind(hdwd, ldply(weather.data$history$dailysummary,
function(x) c('SJC', date.range[i], x$fog, x$rain, x$snow,  x$meantempi, x$meanvism, x$maxtempi, x$mintempi)))
}
colnames(hdwd) <- c("Airport", "Date", 'Fog', 'Rain', 'Snow','AvgTemp', 'AvgVisibility','MaxTemp','MinTemp')
# save to CSV
write.csv(hdwd, file=gzfile('SFC-Jan2006.csv.gz'), row.names=FALSE)
getHistoricalWeather <- function(airport.code="SFO", date="Sys.Date()")
{
base.url <- 'http://api.wunderground.com/api/HSPHDatScience/'
# compose final url
final.url <- paste(base.url, 'history_', date, '/q/', airport.code, '.json', sep='')
# reading in as raw lines from the web service
conn <- url(final.url)
raw.data <- readLines(conn, n=-1L, ok=TRUE)
# Convert to a JSON
weather.data <- fromJSON(paste(raw.data, collapse=""))
close(conn)
return(weather.data)
}
# get data for 10 days - restriction by Weather Underground for free usage
date.range <- seq.Date(from=as.Date('2006-1-01'), to=as.Date('2006-1-10'), by='1 day')
# Initialize a data frame
hdwd <- data.frame()
# loop over dates, and fetch weather data
for(i in seq_along(date.range)) {
weather.data <- getHistoricalWeather('SFO', format(date.range[i], "%Y%m%d"))
hdwd <- rbind(hdwd, ldply(weather.data$history$dailysummary,
function(x) c('SJC', date.range[i], x$fog, x$rain, x$snow,  x$meantempi, x$meanvism, x$maxtempi, x$mintempi)))
}
colnames(hdwd) <- c("Airport", "Date", 'Fog', 'Rain', 'Snow','AvgTemp', 'AvgVisibility','MaxTemp','MinTemp')
# save to CSV
write.csv(hdwd, file=gzfile('SFC-Jan2006.csv.gz'), row.names=FALSE)
getHistoricalWeather <- function(airport.code="SFO", date="Sys.Date()")
{
base.url <- 'http://api.wunderground.com/api/HSPHDatScience/'
# compose final url
final.url <- paste(base.url, 'history_', date, '/q/', airport.code, '.json', sep='')
# reading in as raw lines from the web service
conn <- url(final.url)
raw.data <- readLines(conn, n=-1L, ok=TRUE)
# Convert to a JSON
weather.data <- fromJSON(paste(raw.data, collapse=""))
close(conn)
return(weather.data)
}
# get data for 10 days - restriction by Weather Underground for free usage
date.range <- seq.Date(from=as.Date('2006-1-01'), to=as.Date('2006-1-10'), by='1 day')
# Initialize a data frame
hdwd <- data.frame()
# loop over dates, and fetch weather data
for(i in seq_along(date.range)) {
weather.data <- getHistoricalWeather('SFO', format(date.range[i], "%Y%m%d"))
hdwd <- rbind(hdwd, ldply(weather.data$history$dailysummary,
function(x) c('SJC', date.range[i], x$fog, x$rain, x$snow,  x$meantempi, x$meanvism, x$maxtempi, x$mintempi)))
}
colnames(hdwd) <- c("Airport", "Date", 'Rain', 'Snow','AvgTemp','MaxTemp','MinTemp')
# save to CSV
write.csv(hdwd, file=gzfile('SFC-Jan2006.csv.gz'), row.names=FALSE)
rwunderground/man/forecast10day.Rd
checkDataAvailabilityForDateRange(station_id, start_date, end_date,
station_type = "airportCode")
data_okay <- checkDataAvailabilityForDateRange("BOS",
## End(Not run)
checkSummarizedDataAvailability
"2011-01-01",
"2011-03-31")
data_okay <- checkDataAvailabilityForDateRange("BOS","2011-01-01","2011-03-31")
install.packages("rwunderground")
library(rwunderground)
conditions(set_location(territory = "Hawaii", city = "Honolulu"))
conditions(set_location(territory = "Hawaii", city = "Honolulu"))
install.packages(httr)
install.packages("httr")
library(httr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE)
options(httr_oauth_cache = TRUE)
library(ggplot2)
theme_set(theme_bw())
library(httr)
url <- "https://api.stackexchange.com/2.2/search"
req <- GET(url, query = list(order = "desc",
pagesize = 100,
sort = "creation",
tagged = "r",
site = "stackoverflow"))
req
status_code(req)
con <- content(req, "text")
library(jsonlite)
j <- jsonlite::fromJSON(con)
summary(j)
head(j$items)
library(dplyr)
r_questions <- j$items %>%
flatten() %>%
tbl_df()
r_questions
library(twitteR)
install.packages("twittR")
install.packages("twitteR")
library(twitteR)
options(twitter_consumer_key = "5vJDasKI4ZSUTq2I55kTY1zDT")
options(twitter_consumer_secret = "PGo7miqFwVLakEJ75ahZsrSUFX2akLX2npeMsFQWuiuSllJ675")
options(twitter_access_token = 'https://api.twitter.com/oauth/access_token')
options(twitter_access_token_secret = "https://api.twitter.com/oauth/authorize")
setup_twitter_oauth(getOption("twitter_consumer_key"),
getOption("twitter_consumer_secret"),
getOption("twitter_access_token"),
getOption("twitter_access_token_secret"))
credential <- OAuthFactory$new(consumerKey='5vJDasKI4ZSUTq2I55kTY1zDT',
consumerSecret='PGo7miqFwVLakEJ75ahZsrSUFX2akLX2npeMsFQWuiuSllJ675',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
library(ROAuth)
library(streamR)
credential <- OAuthFactory$new(consumerKey='5vJDasKI4ZSUTq2I55kTY1zDT',
consumerSecret='PGo7miqFwVLakEJ75ahZsrSUFX2akLX2npeMsFQWuiuSllJ675',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
options(twitter_consumer_key = "5vJDasKI4ZSUTq2I55kTY1zDT")
options(twitter_consumer_secret = "PGo7miqFwVLakEJ75ahZsrSUFX2akLX2npeMsFQWuiuSllJ675")
options(twitter_access_token = '1145484565-Uit9nc8ZGCOUqzF7t0iKTcJHHN45t9G8FwyGrlD')
options(twitter_access_token_secret = "3CjrJYFJswCbgMqxPxti9CJaXRL1x6Tdaqfe2CJvR9hLY")
setup_twitter_oauth(getOption("twitter_consumer_key"),
getOption("twitter_consumer_secret"),
getOption("twitter_access_token"),
getOption("twitter_access_token_secret"))
rstats <- searchTwitter("#rstats", n = 500)
# set working directory
# Nina
setwd("~/Documents/HSPH/DataScience/sales-predict-")
library(httr)
library(plyr)
library(dplyr)
library(knitr)
library(readr)
library(tidyr)
library(stringr)
library(ggplot2)
library(gridExtra)
library(ggrepel)
library(rwunderground)
# create date range
date.range <- seq.Date(from=as.Date('2015-6-20'),
to=as.Date('2016-4-20'), by='1 day')
date.range <- str_replace_all(date.range, "[[:punct:]]", "")
ny <- vector(mode='list', length=length(date.range))
for (i in seq_along(date.range)) {
print(date.range[i])
ny[[i]] <- history_daily(set_location(
territory = "New York", city = "New York"), date.range[i])
Sys.sleep(10)
}
# stack loop responses for weather data
weather_NewYork <- ldply(ny)
# save new york data as .csv
write.csv(weather_NewYork, file = "weather_NewYork_06-04_2015-16.csv", row.names=FALSE)
# create date range
date.range <- seq.Date(from=as.Date('2016-4-01'),
to=as.Date('2016-4-20'), by='1 day')
# remove "-" from date range
date.range <- str_replace_all(date.range, "[[:punct:]]", "")
# create empty vector to store dates
ny <- vector(mode='list', length=length(date.range))
# pull weather data from wunderground for all dates in date range
# pause for 10 seconds after each iteration (API only allows 10 requests per minute)
# city = New York
for (i in seq_along(date.range)) {
print(date.range[i])
ny[[i]] <- history_daily(set_location(
territory = "New York", city = "New York"), date.range[i])
Sys.sleep(10)
}
# stack loop responses for weather data
weather_NewYork <- ldply(ny)
# save new york data as .csv
write.csv(weather_NewYork, file = "weather_NewYork_04_2016.csv", row.names=FALSE)
>>>>>>> 26c7738a443879e3a7c0643654fdf555c50c54a4
